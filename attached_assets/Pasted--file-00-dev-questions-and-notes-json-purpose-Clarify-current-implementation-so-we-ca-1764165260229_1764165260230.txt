{
  "file": "00_dev_questions_and_notes.json",
  "purpose": "Clarify current implementation so we can safely add the shared 'brain' layer without breaking existing Flo chat or AI Insights behaviour.",
  "principles": [
    "Do NOT change existing AI Insight prompts or their JSON output format.",
    "Do NOT change existing Grok chat behaviour on failure paths â€“ all new pieces must be additive and non-blocking.",
    "Reuse existing infra (embedding model, job scheduler, logging) wherever possible."
  ],
  "questions_for_dev": [
    {
      "id": 1,
      "topic": "Chat message storage",
      "question": "Is there already a table storing Flo chat transcripts between user and Flo (e.g. flo_chat_messages, chat_logs, conversations)?",
      "details_to_confirm": [
        "If YES, what is the table name and schema? We need at least: user_id, sender (user/flo), message, timestamp.",
        "Is there an index on (user_id, timestamp) to efficiently fetch recent chats for a user?",
        "If NO, we will need to add a simple table, e.g. flo_chat_messages with columns: id (uuid), user_id (uuid, indexed), sender (enum: user/flo), message (text), created_at (timestamptz, indexed)."
      ],
      "non_blocking_requirement": "Writing chat messages to this table must NOT sit on the critical path for returning a chat reply. It should be fire-and-forget or batched so the user still gets fast responses even if logging fails."
    },
    {
      "id": 2,
      "topic": "Existing GPT-based Insights job",
      "question": "Which files currently own the GPT-based AI Insights generation logic? Are insightsEngineV2.ts and insightsSchedulerV2.ts the correct entrypoints?",
      "details_to_confirm": [
        "In insightsEngineV2.ts (or equivalent):",
        " - Does this file assemble the user snapshot (labs, diagnostics, HealthKit data, etc.) and call GPT/LLM to generate insights?",
        " - Does it already produce objects that include Observation, Relevant Metric and Target Metric?",
        "In insightsSchedulerV2.ts (or equivalent):",
        " - Does this orchestrate WHEN insights are generated (cron/queue/schedule)?",
        "If there is a separate dedicated GPT job file (e.g. gptInsightsJob.ts), please identify it. The adapter described in 02_gpt_insights_job.json and 06_metric_and_action_link_extension.json should be attached at the point where the GPT response is already available as JSON."
      ],
      "implementation_note": "The new adapter that writes into user_insights must run AFTER the current insights job has generated its normal output. It must NOT change the existing payload or the UI that consumes it."
    },
    {
      "id": 3,
      "topic": "Embedding model and vector store",
      "question": "Which embedding model is currently used in the codebase (e.g. OpenAI text-embedding-3-small with 1536 dimensions)?",
      "details_to_confirm": [
        "Locate the existing embedding utility (e.g. ai/embeddings.ts, embeddingClient.ts or similar).",
        "Confirm the model name used (for example: text-embedding-3-small).",
        "Confirm the expected vector dimension used by existing vector tables / indexes.",
        "Confirm which vector store / backend is used (pgvector, external service, etc.)."
      ],
      "implementation_note": "The new user_insights_embedding index must reuse the same embedding model and dimension you already use. Do NOT introduce a second embedding configuration unless there is a strong reason. If the current model is text-embedding-3-small with 1536 dimensions, set the user_insights_embedding dimension to 1536 and call the same embedding helper function."
    }
  ],
  "checklist_before_build": [
    "1) Confirm whether a chat transcript table exists and document its schema. If not, design and add flo_chat_messages (or equivalent) in a non-blocking way.",
    "2) Confirm the true entrypoint(s) for the GPT-based Insights engine (insightsEngineV2.ts / insightsSchedulerV2.ts or another file). Mark where the adapter can hook in AFTER the existing insights JSON is produced.",
    "3) Confirm the current embedding model, vector dimension and vector store tech. Align user_insights_embedding to those values.",
    "4) Ensure that all new writes to user_insights and user_insights_embedding are done asynchronously and do not block existing chat or insights responses.",
    "5) Keep all existing prompts and outputs for AI Insights unchanged; the new layer only reads from and adapts what already works."
  ]
}
